{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "datasets = [\n",
    "    \"recist_pancancer_cosmic_card_7-COMBINED\",\n",
    "    \"recist_pancancer_cosmic_card_7-BINARY\",\n",
    "    \"recist_pancancer_union_cosmic_card_7-COMBINED\",\n",
    "    \"recist_pancancer_union_cosmic_card_7-BINARY\",\n",
    "    \"recist_melanoma_cosmic_card_4-COMBINED\",\n",
    "    \"recist_melanoma_cosmic_card_4-BINARY\",\n",
    "    \"recist_melanoma_union_cosmic_card_4-COMBINED\",\n",
    "    \"recist_melanoma_union_cosmic_card_4-BINARY\"\n",
    "    ]\n",
    " \n",
    "json_file = \"../../results/ffs_results_recist_pancancer_union_cosmic_card_7-COMBINED_50.json\"  # \"../../results/ffs_results_recist_pancancer_cosmic_card_7-BINARY_50.json\" \n",
    "output_file =   \"../../data/data_processed/recist_pancancer_union_cosmic_card_7-COMBINED.csv\"  # \"../../data/data_processed/recist_pancancer_cosmic_card_7-BINARY.csv\"\n",
    "\n",
    "# Fix path issues for json_file\n",
    "json_file = os.path.normpath(json_file)\n",
    "print(f\"Using JSON file: {json_file}\")\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    ffs_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(ffs_results)} results from {json_file}\")\n",
    "\n",
    "# Read the melanoma batch corrected data\n",
    "\n",
    "\n",
    "df_melanoma = pd.read_csv(output_file, index_col=0)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df_melanoma.shape}\")\n",
    "print(f\"Columns: {list(df_melanoma.columns[:5])}... (showing first 5)\")\n",
    "print(f\"Target variable (recist) distribution:\")\n",
    "print(df_melanoma['recist'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_melanoma.head())\n",
    "\n",
    "columns = list(df_melanoma.columns[1:])  # Exclude 'Unnamed: 0' and 'recist'\n",
    "print(columns)  # Show first 5 gene names to verify\n",
    "\n",
    "# Check for NaN values in df_melanoma\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR NaN VALUES IN df_melanoma\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for NaN values in the entire dataframe\n",
    "total_nan = df_melanoma.isnull().sum().sum()\n",
    "print(f\"Total NaN values in dataframe: {total_nan}\")\n",
    "\n",
    "# Check NaN values per column\n",
    "nan_per_column = df_melanoma.isnull().sum()\n",
    "columns_with_nan = nan_per_column[nan_per_column > 0]\n",
    "\n",
    "if len(columns_with_nan) > 0:\n",
    "    print(f\"\\nColumns with NaN values:\")\n",
    "    for col, nan_count in columns_with_nan.items():\n",
    "        print(f\"  {col}: {nan_count} NaN values\")\n",
    "else:\n",
    "    print(\"\\nNo NaN values found in any column\")\n",
    "\n",
    "# Specifically check the target variable 'recist'\n",
    "nan_in_target = df_melanoma['recist'].isnull().sum()\n",
    "print(f\"\\nNaN values in target variable 'recist': {nan_in_target}\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_values = np.isinf(df_melanoma.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Infinite values in numeric columns: {inf_values}\")\n",
    "\n",
    "# Show data info\n",
    "print(f\"\\nDataframe shape: {df_melanoma.shape}\")\n",
    "print(f\"Data types:\")\n",
    "print(df_melanoma.dtypes.value_counts())\n",
    "\n",
    "# Remove observations with NaN values in the target variable 'recist'\n",
    "print(f\"Original shape: {df_melanoma.shape}\")\n",
    "df_melanoma_clean = df_melanoma.dropna(subset=['recist'])\n",
    "print(f\"Shape after removing NaN in target: {df_melanoma_clean.shape}\")\n",
    "print(f\"Removed {df_melanoma.shape[0] - df_melanoma_clean.shape[0]} observations with NaN in 'recist'\")\n",
    "\n",
    "# Update the dataframe\n",
    "df_melanoma = df_melanoma_clean\n",
    "\n",
    "# Verify no NaN values remain in target\n",
    "print(f\"NaN values in 'recist' after cleaning: {df_melanoma['recist'].isnull().sum()}\")\n",
    "print(f\"Target distribution after cleaning:\")\n",
    "print(df_melanoma['recist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd64760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True features:\n",
      "[13, 41, 72, 110, 116, 128, 137, 165, 197, 221, 222, 245, 248, 255, 292]\n",
      "Using JSON file: ../../results/ffs_results_mirna_nb_50.json\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Average TPR over runs: 0.7368 ± 0.0506\n",
      "Top 10 most frequent features:\n",
      "[72, 116, 137, 165, 197, 221, 222, 255, 292, 128]\n",
      "\n",
      "Feature frequencies:\n",
      "Feature 72: 19 times\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_k_feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m times\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Map feature indices to actual gene names\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m#top_k_feature_names = [columns[feature] for feature in top_k_features]\u001b[39;00m\n\u001b[32m     74\u001b[39m \n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# Create a summary dataframe\u001b[39;00m\n\u001b[32m     77\u001b[39m     feature_summary = pd.DataFrame({\n\u001b[32m     78\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfeature_index\u001b[39m\u001b[33m'\u001b[39m: top_k_features,\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgene_name\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtop_k_feature_names\u001b[49m\n\u001b[32m     80\u001b[39m     })\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary DataFrame:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(feature_summary)\n",
      "\u001b[31mNameError\u001b[39m: name 'top_k_feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"../../results/ffs_results_mirna_nb_50.json\" #\"../../results/Imvigor_RFE_Results/rfe_results_imvigor_10.json\"  # \"../../results/ffs_results_recist_pancancer_cosmic_card_7-BINARY_50.json\" \n",
    "#output_file =   \"../../data/data_processed/recist_pancancer_union_cosmic_card_7-COMBINED.csv\"  # \"../../data/data_processed/recist_pancancer_cosmic_card_7-BINARY.csv\"\n",
    "\n",
    "json_true = \"../../data/mirna_nbe_info.json\"\n",
    "\n",
    "with open(json_true, \"r\") as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "true_features = info[\"support\"]\n",
    "print(\"True features:\")\n",
    "true_possitions = [idx for idx, val in enumerate(true_features) if val == True]\n",
    "print(true_possitions)\n",
    "import numpy as np\n",
    "\n",
    "# Fix path issues for json_file\n",
    "json_file = os.path.normpath(json_file)\n",
    "print(f\"Using JSON file: {json_file}\")\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    ffs_results = json.load(f)\n",
    "# Extract all selected features from all runs\n",
    "all_features = []\n",
    "TPR = []\n",
    "ii = 0\n",
    "for run_data in ffs_results.values():\n",
    "    # Parse the string representation of the array\n",
    "    feature_str = run_data['selected_features']\n",
    "    \n",
    "    # Remove brackets and split by whitespace\n",
    "    features = feature_str.strip().split()\n",
    "\n",
    "    features = feature_str.replace('[', '').replace(']', '').split()\n",
    "    feature_str = feature_str.replace('[', '').replace(']', '').split()\n",
    "\n",
    "    # Convert to integers\n",
    "    features = [int(f) for f in feature_str]\n",
    "    # Calculate True Positive Rate (TPR)\n",
    "    true_positives = sum(1 for f in features if f in true_possitions)\n",
    "    tpr = true_positives / len(true_possitions) if len(true_possitions) > 0 else 0\n",
    "    TPR.append(tpr)\n",
    "\n",
    "    all_features.extend(features)\n",
    "\n",
    "    ii = ii +1 \n",
    "    print(ii)\n",
    "    if ii == 19:\n",
    "        break\n",
    "\n",
    "print(f\"Average TPR over runs: {np.mean(TPR):.4f} ± {np.std(TPR):.4f}\")\n",
    "# Count frequency of each feature\n",
    "feature_counts = Counter(all_features)\n",
    "\n",
    "# Get top k most frequent features\n",
    "k = 10  # You can change this value\n",
    "top_k_features = [feature for feature, count in feature_counts.most_common(k)]\n",
    "\n",
    "print(f\"Top {k} most frequent features:\")\n",
    "print(top_k_features)\n",
    "\n",
    "# Also show the counts for reference\n",
    "print(f\"\\nFeature frequencies:\")\n",
    "for feature, count in feature_counts.most_common(k):\n",
    "    print(f\"Feature {feature}: {count} times\")\n",
    "\n",
    "\n",
    "\n",
    "    # Map feature indices to actual gene names\n",
    "    #top_k_feature_names = [columns[feature] for feature in top_k_features]\n",
    "\n",
    "\n",
    "    # Create a summary dataframe\n",
    "    feature_summary = pd.DataFrame({\n",
    "        'feature_index': top_k_features,\n",
    "        'gene_name': top_k_feature_names\n",
    "    })\n",
    "\n",
    "print(f\"\\nSummary DataFrame:\")\n",
    "print(feature_summary)\n",
    "\n",
    "print(feature_summary.to_latex(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cca6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def downsample_to_minority_class(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Downsample all classes to the size of the minority class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Target labels\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_downsampled : numpy.ndarray\n",
    "        Downsampled feature matrix\n",
    "    y_downsampled : numpy.ndarray\n",
    "        Downsampled target labels\n",
    "    \"\"\"\n",
    "    # Identify the minority class size\n",
    "    class_counts = pd.Series(y).value_counts()\n",
    "    min_class_size = class_counts.min()\n",
    "    \n",
    "    print(f\"Original class distribution:\")\n",
    "    print(class_counts)\n",
    "    print(f\"\\nMinority class size: {min_class_size}\")\n",
    "    \n",
    "    # Separate by class\n",
    "    indices_by_class = {}\n",
    "    for class_label in class_counts.index:\n",
    "        indices_by_class[class_label] = np.where(y == class_label)[0]\n",
    "    \n",
    "    # Downsample each class to minority class size\n",
    "    downsampled_indices = []\n",
    "    for class_label, indices in indices_by_class.items():\n",
    "        downsampled = resample(indices, \n",
    "                              replace=False, \n",
    "                              n_samples=min_class_size,\n",
    "                              random_state=random_state)\n",
    "        downsampled_indices.extend(downsampled)\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(downsampled_indices)\n",
    "    \n",
    "    # Create downsampled dataset\n",
    "    X_downsampled = X[downsampled_indices]\n",
    "    y_downsampled = y[downsampled_indices]\n",
    "    \n",
    "    print(f\"\\nDownsampled class distribution:\")\n",
    "    print(pd.Series(y_downsampled).value_counts())\n",
    "    print(f\"\\nNew dataset shape: X={X_downsampled.shape}, y={y_downsampled.shape}\")\n",
    "    \n",
    "    return X_downsampled, y_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get top k most frequent features\n",
    "k = 10  # You can change this value\n",
    "for k in range(10, 45):\n",
    "    top_k_features = [feature for feature, count in feature_counts.most_common(k)]\n",
    "\n",
    "    print(f\"Top {k} most frequent features:\")\n",
    "    print(top_k_features)\n",
    "\n",
    "    # Also show the counts for reference\n",
    "    print(f\"\\nFeature frequencies:\")\n",
    "    for feature, count in feature_counts.most_common(k):\n",
    "        print(f\"Feature {feature}: {count} times\")\n",
    "\n",
    "\n",
    "\n",
    "        # Map feature indices to actual gene names\n",
    "        top_k_feature_names = [columns[feature] for feature in top_k_features]\n",
    "\n",
    "\n",
    "        # Create a summary dataframe\n",
    "        feature_summary = pd.DataFrame({\n",
    "            'feature_index': top_k_features,\n",
    "            'gene_name': top_k_feature_names\n",
    "        })\n",
    "\n",
    "    print(f\"\\nSummary DataFrame:\")\n",
    "    print(feature_summary)\n",
    "\n",
    "    print(feature_summary.to_latex(index=False))\n",
    "\n",
    "    # Extract the selected genes from feature_summary\n",
    "    selected_genes = feature_summary['gene_name'].tolist()\n",
    "    print(f\"Selected genes for logistic regression: {selected_genes}\")\n",
    "\n",
    "    # Prepare the data\n",
    "    X = df_melanoma[selected_genes].values\n",
    "    y = df_melanoma['recist'].values\n",
    "\n",
    "    X, y = downsample_to_minority_class(X, y, random_state=42)\n",
    "\n",
    "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"Target distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "    # Create logistic regression model\n",
    "    lr = LogisticRegression( max_iter=1000, class_weight='balanced')\n",
    "\n",
    "    # Perform cross-validation with multiple metrics\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    cv_results = cross_validate(lr, X_scaled, y, cv=cv, scoring=scoring, return_train_score=True)\n",
    "\n",
    "    # Print cross-validation results\n",
    "    print(\"=\"*60)\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for metric in scoring:\n",
    "        train_scores = cv_results[f'train_{metric}']\n",
    "        test_scores = cv_results[f'test_{metric}']\n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"  Train: {train_scores.mean():.3f} ± {train_scores.std():.3f}\")\n",
    "        print(f\"  Test:  {test_scores.mean():.3f} ± {test_scores.std():.3f}\")\n",
    "        print()\n",
    "\n",
    "    # Fit final model on all data for interpretation\n",
    "    lr_final = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_final.fit(X_scaled, y)\n",
    "\n",
    "    # Feature importance (coefficients)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'gene': selected_genes,\n",
    "        'coefficient': lr_final.coef_[0],\n",
    "        'abs_coefficient': np.abs(lr_final.coef_[0])\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE (Logistic Regression Coefficients)\")\n",
    "    print(\"=\"*60)\n",
    "    print(feature_importance)\n",
    "\n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['coefficient'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['gene'], fontsize=8)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Cross-validation scores visualization\n",
    "    plt.subplot(2, 2, 2)\n",
    "    test_scores = [cv_results[f'test_{metric}'] for metric in scoring]\n",
    "    plt.boxplot(test_scores, labels=[m.replace('_macro', '') for m in scoring])\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Cross-Validation Performance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "    # Prediction probabilities for one fold (for illustration)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    lr_temp = LogisticRegression(random_state=42, max_iter=1000,class_weight='balanced')\n",
    "    lr_temp.fit(X_train, y_train)\n",
    "    y_pred_proba = lr_temp.predict_proba(X_test)\n",
    "    y_pred = lr_temp.predict(X_test)\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=lr_temp.classes_, yticklabels=lr_temp.classes_)\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Prediction probability distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for i, class_name in enumerate(lr_temp.classes_):\n",
    "        class_probs = y_pred_proba[:, i]\n",
    "        plt.hist(class_probs, alpha=0.6, label=f'{class_name}', bins=20)\n",
    "    plt.xlabel('Prediction Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Prediction Probability Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(\"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT (Single Train-Test Split)\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of features used: {len(selected_genes)}\")\n",
    "    print(f\"Cross-validation accuracy: {cv_results['test_accuracy'].mean():.3f} ± {cv_results['test_accuracy'].std():.3f}\")\n",
    "    print(f\"Most important features (by |coefficient|):\")\n",
    "    for i, row in feature_importance.head(3).iterrows():\n",
    "        print(f\"  {row['gene']}: {row['coefficient']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get top k most frequent features\n",
    "k = 10  # You can change this value\n",
    "for k in range(13, 34):\n",
    "    top_k_features = [feature for feature, count in feature_counts.most_common(k)]\n",
    "\n",
    "    print(f\"Top {k} most frequent features:\")\n",
    "    print(top_k_features)\n",
    "\n",
    "    # Also show the counts for reference\n",
    "    print(f\"\\nFeature frequencies:\")\n",
    "    for feature, count in feature_counts.most_common(k):\n",
    "        print(f\"Feature {feature}: {count} times\")\n",
    "\n",
    "\n",
    "\n",
    "        # Map feature indices to actual gene names\n",
    "        top_k_feature_names = [columns[feature] for feature in top_k_features]\n",
    "\n",
    "\n",
    "        # Create a summary dataframe\n",
    "        feature_summary = pd.DataFrame({\n",
    "            'feature_index': top_k_features,\n",
    "            'gene_name': top_k_feature_names\n",
    "        })\n",
    "\n",
    "    print(f\"\\nSummary DataFrame:\")\n",
    "    print(feature_summary)\n",
    "\n",
    "    print(feature_summary.to_latex(index=False))\n",
    "\n",
    "    # Extract the selected genes from feature_summary\n",
    "    selected_genes = feature_summary['gene_name'].tolist()\n",
    "    print(f\"Selected genes for logistic regression: {selected_genes}\")\n",
    "\n",
    "    # Prepare the data\n",
    "    X = df_melanoma[selected_genes].values\n",
    "    y = df_melanoma['recist'].values\n",
    "    y_combined = np.where((y == 'PR-SD') | (y == 'PD'), 'PD-PR-SD', y)\n",
    "\n",
    "    X, y_combined = downsample_to_minority_class(X, y_combined, random_state=42)\n",
    "    \n",
    "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"Target distribution: {pd.Series(y_combined).value_counts().to_dict()}\")\n",
    "\n",
    "    # Standardize features\n",
    "    #scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "    # Create logistic regression model\n",
    "    lr = LogisticRegression( max_iter=1000, class_weight='balanced')\n",
    "\n",
    "    # Perform cross-validation with multiple metrics\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    cv_results = cross_validate(lr, X_scaled, y_combined, cv=cv, scoring=scoring, return_train_score=True)\n",
    "\n",
    "    # Print cross-validation results\n",
    "    print(\"=\"*60)\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for metric in scoring:\n",
    "        train_scores = cv_results[f'train_{metric}']\n",
    "        test_scores = cv_results[f'test_{metric}']\n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"  Train: {train_scores.mean():.3f} ± {train_scores.std():.3f}\")\n",
    "        print(f\"  Test:  {test_scores.mean():.3f} ± {test_scores.std():.3f}\")\n",
    "        print()\n",
    "\n",
    "    # Fit final model on all data for interpretation\n",
    "    lr_final = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "    lr_final.fit(X_scaled, y_combined)\n",
    "\n",
    "    # Feature importance (coefficients)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'gene': selected_genes,\n",
    "        'coefficient': lr_final.coef_[0],\n",
    "        'abs_coefficient': np.abs(lr_final.coef_[0])\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE (Logistic Regression Coefficients)\")\n",
    "    print(\"=\"*60)\n",
    "    print(feature_importance)\n",
    "\n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['coefficient'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['gene'], fontsize=8)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Cross-validation scores visualization\n",
    "    plt.subplot(2, 2, 2)\n",
    "    test_scores = [cv_results[f'test_{metric}'] for metric in scoring]\n",
    "    plt.boxplot(test_scores, labels=[m.replace('_macro', '') for m in scoring])\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Cross-Validation Performance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "    # Prediction probabilities for one fold (for illustration)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=42, stratify=y_combined)\n",
    "    lr_temp = LogisticRegression( max_iter=1000)\n",
    "    lr_temp.fit(X_train, y_train)\n",
    "    y_pred_proba = lr_temp.predict_proba(X_test)\n",
    "    y_pred = lr_temp.predict(X_test)\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=lr_temp.classes_, yticklabels=lr_temp.classes_)\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Prediction probability distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "   \n",
    "\n",
    "    # Elegimos la probabilidad de la clase mayoritaria (o la positiva, index 1 generalmente)\n",
    "    # Asumimos que lr_temp.classes_[1] es 'PD-PR-SD'\n",
    "    pos_class_index = 1 \n",
    "    probs_pos_class = y_pred_proba[:, pos_class_index]\n",
    "\n",
    "    # Separamos las probabilidades basadas en la verdad (y_test)\n",
    "    for class_label in lr_temp.classes_:\n",
    "        # Buscamos los índices donde la etiqueta REAL es class_label\n",
    "        mask = (y_test == class_label)\n",
    "        \n",
    "        # Graficamos las probabilidades SOLO de esos pacientes\n",
    "        plt.hist(probs_pos_class[mask], alpha=0.6, label=f'True {class_label}', bins=10)\n",
    "\n",
    "    plt.xlabel(f'Probability of being {lr_temp.classes_[pos_class_index]}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Separation of Classes')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(\"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT (Single Train-Test Split)\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of features used: {len(selected_genes)}\")\n",
    "    print(f\"Cross-validation accuracy: {cv_results['test_accuracy'].mean():.3f} ± {cv_results['test_accuracy'].std():.3f}\")\n",
    "    print(f\"Most important features (by |coefficient|):\")\n",
    "    for i, row in feature_importance.head(3).iterrows():\n",
    "        print(f\"  {row['gene']}: {row['coefficient']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71865f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
