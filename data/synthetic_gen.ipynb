{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e02a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "synthetic_feature_benchmark.py  (v2, multiclass + miRNA NB simulator)\n",
    "\n",
    "Generador de datasets sintéticos con soporte (variables relevantes) conocido.\n",
    "Pensado para evaluar selectores de atributos (feature selection).\n",
    "\n",
    "Novedades:\n",
    "- **Clasificación multiclase** (softmax) para modelos de clasificación ('sparse_logistic', 'xor', 'parity', 'madelon_like').\n",
    "- **Nuevo modelo 'mirna_nb'**: simulación de conteos de miRNA/trascriptómica con\n",
    "  distribución Binomial Negativa (gamma-poisson), tamaños de biblioteca, genes/miRNAs\n",
    "  diferencialmente expresados entre clases y módulos de coexpresión opcionales.\n",
    "\n",
    "Recetarios (model):\n",
    "- 'sparse_logistic'  -> Clasificación (K clases) con soporte escaso, X gaussiano correlado.\n",
    "- 'sparse_linear'    -> Regresión lineal esparsa.\n",
    "- 'friedman1'        -> Regresión no lineal clásica (Friedman #1).\n",
    "- 'xor' / 'parity'   -> Clasificación (K clases) por interacciones puras.\n",
    "- 'madelon_like'     -> Clasificación (K clases) estilo MADELON (clústeres en hipercubo).\n",
    "- 'mirna_nb'         -> Clasificación (K clases) con **datos de conteo** tipo miRNA (NB).\n",
    "\n",
    "Uso rápido:\n",
    "    from synthetic_feature_benchmark import make_benchmark_dataset\n",
    "\n",
    "    X, y, info = make_benchmark_dataset(\n",
    "        n_samples=1000, n_features=200, n_informative=10,\n",
    "        model=\"sparse_logistic\", corr=\"toeplitz\", rho=0.5,\n",
    "        n_classes=3, class_probs=[0.2, 0.5, 0.3], random_state=42\n",
    "    )\n",
    "\n",
    "    Xc, yc, info_c = make_benchmark_dataset(\n",
    "        n_samples=600, n_features=300, n_informative=40,\n",
    "        model=\"mirna_nb\", n_classes=3, class_probs=[1/3,1/3,1/3],\n",
    "        effect_size=0.8, # log-fold-change aproximado\n",
    "        mirna_library_log_mean=11.0, mirna_library_log_sd=0.5,\n",
    "        mirna_dispersion_scale=1.0, n_modules=5, module_sd=0.3,\n",
    "        random_state=7\n",
    "    )\n",
    "\n",
    "Devuelve:\n",
    "- X: np.ndarray (n_samples, n_features)  (float en la mayoría de modelos, **int** para 'mirna_nb')\n",
    "- y: np.ndarray (n_samples,) con etiquetas 0..K-1 (si clasificación) o valores reales (regresión)\n",
    "- info: dict con metadatos:\n",
    "        - 'support': máscara booleana de variables estrictamente informativas\n",
    "        - 'support_with_redundant': máscara booleana incluyendo redundantes\n",
    "        - 'coef' o 'coef_matrix' (si aplica)\n",
    "        - 'model', 'n_classes', 'class_probs', 'snr', 'noise_std', 'rho', etc.\n",
    "        - 'redundant_map': lista de tuplas (j_redundante, indices_fuente)\n",
    "        - 'repeated_from': lista de tuplas (j_repetida, j_original)\n",
    "        - 'categorical_idx': índices de columnas categóricas (si se crean)\n",
    "        - 'is_counts': True para 'mirna_nb'; además incluye 'library_sizes', 'dispersion', 'de_log_fc'\n",
    "        - 'notes': texto con detalles\n",
    "\n",
    "Requisitos: NumPy.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, Any, List, Sequence\n",
    "import numpy as np\n",
    "\n",
    "Array = np.ndarray\n",
    "\n",
    "# ===================== Utilidades generales =====================\n",
    "\n",
    "def _rng(random_state: Optional[int] = None) -> np.random.Generator:\n",
    "    if isinstance(random_state, np.random.Generator):\n",
    "        return random_state\n",
    "    return np.random.default_rng(random_state)\n",
    "\n",
    "def _toeplitz_cov(p: int, rho: float) -> Array:\n",
    "    idx = np.arange(p)\n",
    "    return rho ** np.abs(idx[:, None] - idx[None, :])\n",
    "\n",
    "def _block_equicorr_cov(p: int, n_blocks: int, rho_within: float, rho_between: float = 0.0) -> Array:\n",
    "    if n_blocks <= 0:\n",
    "        raise ValueError(\"n_blocks debe ser >= 1\")\n",
    "    sizes = np.full(n_blocks, p // n_blocks, dtype=int)\n",
    "    sizes[: p % n_blocks] += 1\n",
    "    idxs = np.cumsum(np.r_[0, sizes])\n",
    "    Σ = np.full((p, p), rho_between, dtype=float)\n",
    "    for b in range(n_blocks):\n",
    "        a, bnd = idxs[b], idxs[b+1]\n",
    "        Σ[a:bnd, a:bnd] = rho_within\n",
    "    np.fill_diagonal(Σ, 1.0)\n",
    "    eigmin = np.linalg.eigvalsh(Σ).min()\n",
    "    if eigmin <= 1e-6:\n",
    "        Σ += (1e-6 - eigmin + 1e-8) * np.eye(p)\n",
    "    return Σ\n",
    "\n",
    "def _sample_gaussian_features(n: int, p: int, corr: str, rho: float, n_blocks: int, rng: np.random.Generator) -> Array:\n",
    "    if corr == \"independent\":\n",
    "        X = rng.standard_normal((n, p))\n",
    "    elif corr == \"toeplitz\":\n",
    "        Σ = _toeplitz_cov(p, rho)\n",
    "        L = np.linalg.cholesky(Σ)\n",
    "        X = rng.standard_normal((n, p)) @ L.T\n",
    "    elif corr == \"block\":\n",
    "        Σ = _block_equicorr_cov(p, n_blocks=n_blocks, rho_within=rho, rho_between=0.0)\n",
    "        L = np.linalg.cholesky(Σ)\n",
    "        X = rng.standard_normal((n, p)) @ L.T\n",
    "    else:\n",
    "        raise ValueError(f\"corr '{corr}' no soportado. Use 'independent', 'toeplitz' o 'block'.\")\n",
    "    return X\n",
    "\n",
    "def _build_support(p: int, n_informative: int, rng: np.random.Generator, support: Optional[np.ndarray]) -> np.ndarray:\n",
    "    if support is not None:\n",
    "        support = np.asarray(support, dtype=bool)\n",
    "        if support.shape[0] != p:\n",
    "            raise ValueError(\"La máscara 'support' debe tener longitud n_features\")\n",
    "        if support.sum() != n_informative:\n",
    "            n_informative = int(support.sum())\n",
    "        return support\n",
    "    idx = rng.choice(p, size=min(n_informative, p), replace=False)\n",
    "    sup = np.zeros(p, dtype=bool)\n",
    "    sup[idx] = True\n",
    "    return sup\n",
    "\n",
    "def _scale_noise_for_snr(signal: Array, snr: float) -> float:\n",
    "    var_signal = float(np.var(signal))\n",
    "    if snr <= 0:\n",
    "        raise ValueError(\"snr debe ser > 0\")\n",
    "    sigma = math.sqrt(max(var_signal, 1e-12) / snr)\n",
    "    return sigma\n",
    "\n",
    "def _bisect_intercept(eta: Array, target_pi: float, tol: float = 1e-6, max_iter: int = 200) -> float:\n",
    "    lo, hi = -40.0, 40.0\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        p = 1.0 / (1.0 + np.exp(-(eta + mid)))\n",
    "        m = p.mean()\n",
    "        if abs(m - target_pi) < tol:\n",
    "            return float(mid)\n",
    "        if m > target_pi:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return float(mid)\n",
    "\n",
    "def _softmax(Z: Array) -> Array:\n",
    "    # Z: (n, K)\n",
    "    Z = Z - Z.max(axis=1, keepdims=True)\n",
    "    expZ = np.exp(Z)\n",
    "    return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "\n",
    "def _calibrate_softmax_intercepts(eta_no_b: Array, target_probs: Array,\n",
    "                                  max_iter: int = 500, tol: float = 1e-7, lr: float = 1.0) -> Array:\n",
    "    \"\"\"\n",
    "    Ajusta un vector de interceptos b (longitud K) tal que\n",
    "    mean(softmax(eta_no_b + b)) ≈ target_probs.\n",
    "    \"\"\"\n",
    "    n, K = eta_no_b.shape\n",
    "    b = np.zeros(K, dtype=float)\n",
    "    target = np.asarray(target_probs, dtype=float)\n",
    "    target = target / target.sum()\n",
    "    for _ in range(max_iter):\n",
    "        P = _softmax(eta_no_b + b)\n",
    "        grad = P.mean(axis=0) - target     # dL/db ~ diferencia de promedios\n",
    "        gnorm = np.linalg.norm(grad, ord=2)\n",
    "        if gnorm < tol:\n",
    "            break\n",
    "        # paso con pequeño damping\n",
    "        b -= lr * grad\n",
    "        # retirar la media para evitar indeterminación (sum shift invariance)\n",
    "        b -= b.mean()\n",
    "    return b\n",
    "\n",
    "def _add_redundant_features(X: Array, support_idx: np.ndarray, n_redundant: int,\n",
    "                            noise_std: float, rng: np.random.Generator):\n",
    "    n, p = X.shape\n",
    "    red_map = []\n",
    "    if n_redundant <= 0:\n",
    "        return X, red_map\n",
    "    base_idx = np.array(support_idx, copy=False)\n",
    "    for _ in range(n_redundant):\n",
    "        k = int(rng.integers(low=1, high=min(4, len(base_idx)) + 1))\n",
    "        choose = rng.choice(base_idx, size=k, replace=False)\n",
    "        weights = rng.normal(0, 1, size=k)\n",
    "        new_col = X[:, choose] @ weights + rng.normal(0, noise_std * 0.1, size=n)\n",
    "        X = np.column_stack([X, new_col])\n",
    "        red_map.append((X.shape[1]-1, list(map(int, choose))))\n",
    "    return X, red_map\n",
    "\n",
    "def _add_repeated_features(X: Array, n_repeats: int, rng: np.random.Generator):\n",
    "    n, p = X.shape\n",
    "    rep = []\n",
    "    if n_repeats <= 0:\n",
    "        return X, rep\n",
    "    src = rng.integers(low=0, high=p, size=n_repeats)\n",
    "    for s in src:\n",
    "        X = np.column_stack([X, X[:, s]])\n",
    "        rep.append((X.shape[1]-1, int(s)))\n",
    "    return X, rep\n",
    "\n",
    "def _add_categorical_noise(X: Array, n_categorical: int, n_categories: int, rng: np.random.Generator):\n",
    "    n, _ = X.shape\n",
    "    cat_idx = []\n",
    "    for _ in range(n_categorical):\n",
    "        col = rng.integers(0, n_categories, size=n).astype(float)\n",
    "        X = np.column_stack([X, col])\n",
    "        cat_idx.append(X.shape[1]-1)\n",
    "    return X, cat_idx\n",
    "\n",
    "# ===================== Utilidades NB (miRNA/scRNA) =====================\n",
    "\n",
    "def _sample_nb_means(n_genes: int, rng: np.random.Generator,\n",
    "                     mean_log_expr: float = 1.5, sd_log_expr: float = 1.0) -> Array:\n",
    "    # Intensidades base por gen (proporciones) ~ LogNormal\n",
    "    base_int = rng.lognormal(mean=mean_log_expr, sigma=sd_log_expr, size=n_genes)\n",
    "    base_int = np.maximum(base_int, 1e-12)\n",
    "    pi_g = base_int / base_int.sum()  # proporciones por gen\n",
    "    return pi_g\n",
    "\n",
    "def _sample_library_sizes(n_samples: int, rng: np.random.Generator,\n",
    "                          log_mean: float = 11.0, log_sd: float = 0.5) -> Array:\n",
    "    # Tamaños de biblioteca (total counts por célula/muestra) ~ LogNormal\n",
    "    L = rng.lognormal(mean=log_mean, sigma=log_sd, size=n_samples)\n",
    "    return L\n",
    "\n",
    "def _sample_nb_counts(mu: Array, disp: Array, rng: np.random.Generator) -> Array:\n",
    "    \"\"\"\n",
    "    Muestras de NB con parametrización (mean=mu, dispersion=alpha),\n",
    "    Var = mu + alpha * mu^2. Se implementa como Gamma-Poisson:\n",
    "      lambda ~ Gamma(shape=1/alpha, scale=mu*alpha),  y ~ Poisson(lambda).\n",
    "    - mu: (n, p)\n",
    "    - disp: (p,)  (alpha >= 0)\n",
    "    \"\"\"\n",
    "    n, p = mu.shape\n",
    "    alpha = np.asarray(disp, dtype=float)\n",
    "    alpha = np.maximum(alpha, 1e-12)\n",
    "    r = 1.0 / alpha\n",
    "    # Broadcasting shape/scale por columna\n",
    "    lam = rng.gamma(shape=r, scale=mu * alpha, size=(n, p))\n",
    "    y = rng.poisson(lam)\n",
    "    return y\n",
    "\n",
    "# ===================== Interfaz principal =====================\n",
    "\n",
    "def make_benchmark_dataset(\n",
    "    n_samples: int = 1000,\n",
    "    n_features: int = 100,\n",
    "    n_informative: int = 10,\n",
    "    # Tarea y modelo\n",
    "    task: str = \"classification\",            # Se infiere a partir de 'model'; aquí es informativo\n",
    "    model: str = \"sparse_logistic\",          # 'sparse_logistic', 'sparse_linear', 'friedman1', 'xor', 'parity', 'madelon_like', 'mirna_nb'\n",
    "    # Estructura de X (gaussiano)\n",
    "    corr: str = \"independent\",               # 'independent', 'toeplitz', 'block'\n",
    "    rho: float = 0.3,                        # intensidad de correlación (Toeplitz/Block)\n",
    "    n_blocks: int = 5,                       # nº de bloques si corr='block'\n",
    "    # Señal/ruido y efectos\n",
    "    snr: float = 5.0,                        # SNR para regresión; cuantos \"signal variances\" por \"noise variance\"\n",
    "    effect_size: float = 1.0,                # magnitud de coeficientes (lineal) o log-fold-change (miRNA)\n",
    "    heteroscedastic: float = 0.0,            # 0 => homoscedástico; >0 => var(noise) crece con |Xβ|\n",
    "    # Clasificación\n",
    "    n_classes: int = 2,\n",
    "    class_balance: float = 0.5,              # mantenido por compatibilidad (binario); usar 'class_probs' preferentemente\n",
    "    class_probs: Optional[Sequence[float]] = None,  # distribución objetivo de clases\n",
    "    # Redundantes/ruido extra\n",
    "    n_redundant: int = 0,\n",
    "    n_repeated: int = 0,\n",
    "    n_categorical: int = 0,\n",
    "    n_categories: int = 3,\n",
    "    # Opcional: fijar soporte\n",
    "    support_mask: Optional[np.ndarray] = None,\n",
    "    # Parámetros específicos miRNA\n",
    "    mirna_library_log_mean: float = 11.0,\n",
    "    mirna_library_log_sd: float = 0.5,\n",
    "    mirna_mean_log_expr: float = 1.5,\n",
    "    mirna_sd_log_expr: float = 1.0,\n",
    "    mirna_dispersion_scale: float = 1.0,     # escala para la dispersión (alpha)\n",
    "    dropout_rate: float = 0.0,               # prob. de \"dropout\" (0 típico para miRNA; >0 para simular scRNA)\n",
    "    n_modules: int = 0,                      # nº de módulos de coexpresión (0 => sin módulos)\n",
    "    module_sd: float = 0.3,                  # desviación (log-escala) de factores por muestra para cada módulo\n",
    "    random_state: Optional[int] = None\n",
    ") -> Tuple[Array, Array, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Crea un dataset sintético con soporte conocido de variables relevantes.\n",
    "    Permite clasificación multiclase en modelos de clasificación, y un modelo\n",
    "    específico de conteos NB para miRNA ('mirna_nb').\n",
    "\n",
    "    Retorna (X, y, info).\n",
    "    \"\"\"\n",
    "    rng = _rng(random_state)\n",
    "\n",
    "    # ====== Preparación de probabilidades de clase ======\n",
    "    if class_probs is None:\n",
    "        if n_classes == 2:\n",
    "            class_probs = [1 - class_balance, class_balance]\n",
    "        else:\n",
    "            class_probs = [1.0 / n_classes] * n_classes\n",
    "    class_probs = np.asarray(class_probs, dtype=float)\n",
    "    if len(class_probs) != n_classes:\n",
    "        raise ValueError(\"class_probs debe tener longitud n_classes\")\n",
    "    class_probs = class_probs / class_probs.sum()\n",
    "\n",
    "    # ====== MODELOS ESPECÍFICOS ======\n",
    "    if model in (\"sparse_logistic\", \"sparse_linear\"):\n",
    "        # 1) Generamos X\n",
    "        X = _sample_gaussian_features(n_samples, n_features, corr=corr, rho=rho, n_blocks=n_blocks, rng=rng)\n",
    "\n",
    "        # 2) Soporte\n",
    "        support = _build_support(n_features, n_informative, rng, support_mask)\n",
    "\n",
    "        if model == \"sparse_linear\":\n",
    "            # Coeficientes sparse para regresión\n",
    "            coef = np.zeros(n_features)\n",
    "            magnitudes = rng.uniform(0.5, 1.5, size=support.sum()) * effect_size\n",
    "            signs = rng.choice([-1.0, 1.0], size=support.sum())\n",
    "            coef[support] = magnitudes * signs\n",
    "            eta = X @ coef\n",
    "            sigma = _scale_noise_for_snr(eta, snr=snr)\n",
    "            if heteroscedastic > 0:\n",
    "                noise = rng.normal(0, sigma, size=n_samples) * (1.0 + heteroscedastic * np.abs(eta))\n",
    "            else:\n",
    "                noise = rng.normal(0, sigma, size=n_samples)\n",
    "            y = eta + noise\n",
    "            base_noise_std = float(np.std(eta))\n",
    "\n",
    "            info = {\n",
    "                \"model\": model, \"task\": \"regression\",\n",
    "                \"support\": support, \"support_with_redundant\": support.copy(),\n",
    "                \"coef\": coef, \"coef_matrix\": None,\n",
    "                \"snr\": snr, \"noise_std\": base_noise_std, \"corr\": corr, \"rho\": rho, \"n_blocks\": n_blocks,\n",
    "                \"n_classes\": None, \"class_probs\": None,\n",
    "                \"notes\": \"Diseño gaussiano con soporte escaso; coeficientes aleatorios (regresión).\"\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Clasificación multiclase (softmax) con coeficientes compartiendo el mismo soporte\n",
    "            K = int(n_classes)\n",
    "            # Matriz de coeficientes (p x K); ceros fuera del soporte\n",
    "            coef_matrix = np.zeros((n_features, K))\n",
    "            magnitudes = rng.uniform(0.5, 1.5, size=(support.sum(), K)) * effect_size\n",
    "            signs = rng.choice([-1.0, 1.0], size=(support.sum(), K))\n",
    "            coef_matrix[support, :] = magnitudes * signs\n",
    "            eta_no_b = X @ coef_matrix  # (n, K)\n",
    "            # Calibrar interceptos para alcanzar class_probs deseadas\n",
    "            b = _calibrate_softmax_intercepts(eta_no_b, class_probs)\n",
    "            eta = eta_no_b + b  # broadcasting\n",
    "            P = _softmax(eta)\n",
    "            # Muestreamos clases\n",
    "            cumP = np.cumsum(P, axis=1)\n",
    "            r = rng.random(n_samples)[:, None]\n",
    "            y = (cumP < r).sum(axis=1)\n",
    "            info = {\n",
    "                \"model\": model, \"task\": \"classification\",\n",
    "                \"support\": support, \"support_with_redundant\": support.copy(),\n",
    "                \"coef\": None, \"coef_matrix\": coef_matrix,\n",
    "                \"snr\": None, \"noise_std\": float(np.std(eta_no_b)),\n",
    "                \"corr\": corr, \"rho\": rho, \"n_blocks\": n_blocks,\n",
    "                \"n_classes\": K, \"class_probs\": class_probs,\n",
    "                \"notes\": \"Clasificación softmax con soporte escaso y X gaussiano.\"\n",
    "            }\n",
    "\n",
    "    elif model == \"friedman1\":\n",
    "        p0 = max(n_features, 10)\n",
    "        X = rng.uniform(0.0, 1.0, size=(n_samples, p0))\n",
    "        y_signal = 10*np.sin(np.pi * X[:,0] * X[:,1]) + 20*(X[:,2] - 0.5)**2 + 10*X[:,3] + 5*X[:,4]\n",
    "        sigma = _scale_noise_for_snr(y_signal, snr=snr)\n",
    "        y = y_signal + rng.normal(0, sigma, size=n_samples)\n",
    "        support = np.zeros(p0, dtype=bool); support[:5] = True\n",
    "        info = {\n",
    "            \"model\": \"friedman1\", \"task\": \"regression\",\n",
    "            \"support\": support[:n_features], \"support_with_redundant\": support[:n_features].copy(),\n",
    "            \"coef\": None, \"coef_matrix\": None, \"snr\": snr, \"noise_std\": sigma,\n",
    "            \"corr\": \"independent\", \"rho\": 0.0, \"n_blocks\": None,\n",
    "            \"n_classes\": None, \"class_probs\": None,\n",
    "            \"notes\": \"Friedman #1: no lineal aditivo con 5 variables relevantes (x1..x5).\"\n",
    "        }\n",
    "        if p0 != n_features:\n",
    "            if n_features < p0:\n",
    "                X = X[:, :n_features]\n",
    "            else:\n",
    "                extra = rng.uniform(0.0, 1.0, size=(n_samples, n_features - p0))\n",
    "                X = np.concatenate([X, extra], axis=1)\n",
    "\n",
    "    elif model in (\"xor\", \"parity\"):\n",
    "        k = int(max(2, n_informative))\n",
    "        bits = rng.integers(0, 2, size=(n_samples, k))\n",
    "        jitter = rng.normal(0, 0.1, size=(n_samples, k))\n",
    "        informative = bits.astype(float) + jitter\n",
    "        p_noise = max(0, n_features - k)\n",
    "        noise = rng.standard_normal((n_samples, p_noise)) if p_noise > 0 else None\n",
    "        X = np.concatenate([informative, noise], axis=1) if noise is not None else informative\n",
    "        # Etiquetas multiclase: suma de bits modulo K\n",
    "        K = int(n_classes)\n",
    "        y = (bits.sum(axis=1) % K).astype(int)\n",
    "        support = np.zeros(X.shape[1], dtype=bool); support[:k] = True\n",
    "        info = {\n",
    "            \"model\": model, \"task\": \"classification\",\n",
    "            \"support\": support, \"support_with_redundant\": support.copy(),\n",
    "            \"coef\": None, \"coef_matrix\": None, \"snr\": None, \"noise_std\": 0.1,\n",
    "            \"corr\": \"independent\", \"rho\": 0.0, \"n_blocks\": None,\n",
    "            \"n_classes\": K, \"class_probs\": class_probs,\n",
    "            \"notes\": \"Etiqueta = (paridad/XOR de k bits) mod K, con jitter.\"\n",
    "        }\n",
    "\n",
    "    elif model == \"madelon_like\":\n",
    "        base_dim = 5\n",
    "        vertices = np.array(np.meshgrid(*[[-1, 1]]*base_dim)).T.reshape(-1, base_dim)\n",
    "        # Asignación de clases a los 32 vértices, intentando balancear K clases\n",
    "        K = int(n_classes)\n",
    "        labels = np.repeat(np.arange(K), repeats=len(vertices)//K)\n",
    "        if labels.size < len(vertices):\n",
    "            labels = np.r_[labels, np.arange(len(vertices) - labels.size) % K]\n",
    "        labels = labels[:len(vertices)]\n",
    "        labels = rng.permutation(labels)  # aleatorizar asignación vértice->clase\n",
    "        # Generación por clúster\n",
    "        centers_idx = rng.integers(0, len(vertices), size=n_samples)\n",
    "        centers = vertices[centers_idx]\n",
    "        y = labels[centers_idx]\n",
    "        sep = 2.0\n",
    "        base = centers * sep + rng.normal(0, 1.0, size=(n_samples, base_dim))\n",
    "        # Combinaciones lineales (redundantes) de las 5 bases\n",
    "        n_lin = min(15, max(0, n_features - base_dim))\n",
    "        lin_cols = []\n",
    "        red_map = []\n",
    "        for j in range(n_lin):\n",
    "            w = rng.normal(0, 1, size=base_dim)\n",
    "            col = base @ w + rng.normal(0, 0.2, size=n_samples)\n",
    "            lin_cols.append(col)\n",
    "        if lin_cols:\n",
    "            lin_cols = np.column_stack(lin_cols)\n",
    "            X = np.column_stack([base, lin_cols])\n",
    "            red_map = [(base_dim + j, list(range(base_dim))) for j in range(n_lin)]\n",
    "        else:\n",
    "            X = base\n",
    "        if X.shape[1] < n_features:\n",
    "            X = np.column_stack([X, rng.standard_normal((n_samples, n_features - X.shape[1]))])\n",
    "        support = np.zeros(X.shape[1], dtype=bool); support[:base_dim] = True\n",
    "        support_with_redundant = support.copy()\n",
    "        support_with_redundant[base_dim:base_dim + n_lin] = True\n",
    "        info = {\n",
    "            \"model\": \"madelon_like\", \"task\": \"classification\",\n",
    "            \"support\": support, \"support_with_redundant\": support_with_redundant,\n",
    "            \"coef\": None, \"coef_matrix\": None, \"snr\": None, \"noise_std\": 1.0,\n",
    "            \"corr\": \"independent\", \"rho\": 0.0, \"n_blocks\": None,\n",
    "            \"redundant_map\": red_map,\n",
    "            \"n_classes\": K, \"class_probs\": class_probs,\n",
    "            \"notes\": f\"Clústeres en hipercubo 5D; 5 informativas + {n_lin} combinaciones lineales + ruido, con {K} clases.\"\n",
    "        }\n",
    "\n",
    "    elif model == \"mirna_nb\":\n",
    "        # ===== Simulación de conteos NB (miRNA) =====\n",
    "        K = int(n_classes)\n",
    "        # 1) Asignar clases a muestras según class_probs\n",
    "        cum = np.cumsum(class_probs)\n",
    "        r = rng.random(n_samples)\n",
    "        y = np.searchsorted(cum, r).astype(int)\n",
    "\n",
    "        p = n_features\n",
    "        # 2) Proporciones base por gen/miRNA\n",
    "        pi_g = _sample_nb_means(p, rng, mean_log_expr=mirna_mean_log_expr, sd_log_expr=mirna_sd_log_expr)  # (p,)\n",
    "        # 3) Tamaños de biblioteca por muestra\n",
    "        L = _sample_library_sizes(n_samples, rng, log_mean=mirna_library_log_mean, log_sd=mirna_library_log_sd)  # (n,)\n",
    "        # 4) Módulos de coexpresión opcionales (en log-escala multiplicativa)\n",
    "        module_of = None\n",
    "        sample_module_effect = None\n",
    "        if n_modules and n_modules > 0:\n",
    "            module_of = rng.integers(0, n_modules, size=p)\n",
    "            sample_module_effect = rng.normal(0.0, module_sd, size=(n_samples, n_modules))\n",
    "        # 5) Soporte DE: genes informativos (DE en al menos una clase)\n",
    "        support = _build_support(p, n_informative, rng, support_mask)\n",
    "        # 6) Efectos por clase (log-fold-changes); clase 0 referencia (0)\n",
    "        de_log_fc = np.zeros((p, K), dtype=float)\n",
    "        # Para genes informativos, aplicar efectos aleatorios por clase (centrados)\n",
    "        for g in np.flatnonzero(support):\n",
    "            # efectos por clase alrededor de 0, con escala effect_size\n",
    "            de_log_fc[g, 1:] = rng.normal(0.0, effect_size, size=K-1)\n",
    "            # opcional: hacer algunos efectos con signo consistente por clase\n",
    "        # 7) Medias mu_{i,g} = L_i * pi_g * exp(module_effect + class_effect)\n",
    "        log_mu = (np.log(L)[:, None] + np.log(pi_g)[None, :])  # (n,p)\n",
    "        if n_modules and n_modules > 0:\n",
    "            # sumar efecto del módulo correspondiente a cada gen\n",
    "            m_idx = module_of[None, :]                 # (1,p)\n",
    "            mod_eff = np.take_along_axis(sample_module_effect, m_idx, axis=1)  # (n,p)\n",
    "            log_mu = log_mu + mod_eff\n",
    "        # efecto por clase sobre genes informativos\n",
    "        class_eff = de_log_fc[y, :]                    # (n,K) but we need per gene -> expand\n",
    "        # Convertir a matriz (n,p): por muestra, añadir efecto de su clase para cada gen\n",
    "        # de_log_fc es (p,K); tomamos columna y_i\n",
    "        class_shift_per_gene = de_log_fc[:, y].T      # (n,p)\n",
    "        log_mu = log_mu + class_shift_per_gene\n",
    "        mu = np.exp(log_mu)                            # medias NB\n",
    "        # 8) Dispersión por gen: alpha >= 0 (Var = mu + alpha*mu^2)\n",
    "        #    Usamos alpha_g ~ LogNormal(-1.5, 0.5) escalada\n",
    "        alpha_g = np.exp(rng.normal(-1.5, 0.5, size=p)) * float(mirna_dispersion_scale)\n",
    "        # 9) Muestras de conteo\n",
    "        X = _sample_nb_counts(mu, alpha_g, rng)\n",
    "        # 10) Dropout opcional\n",
    "        if dropout_rate and dropout_rate > 0.0:\n",
    "            mask = rng.random(size=X.shape) < float(dropout_rate)\n",
    "            X[mask] = 0\n",
    "        info = {\n",
    "            \"model\": \"mirna_nb\", \"task\": \"classification\",\n",
    "            \"support\": support, \"support_with_redundant\": support.copy(),\n",
    "            \"coef\": None, \"coef_matrix\": None,\n",
    "            \"snr\": None, \"noise_std\": None, \"corr\": None, \"rho\": None, \"n_blocks\": None,\n",
    "            \"n_classes\": K, \"class_probs\": class_probs,\n",
    "            \"is_counts\": True,\n",
    "            \"library_sizes\": L, \"base_proportions\": pi_g,\n",
    "            \"dispersion\": alpha_g, \"de_log_fc\": de_log_fc,\n",
    "            \"module_of\": module_of, \"module_sd\": module_sd,\n",
    "            \"notes\": \"Conteos NB por miRNA con tamaños de biblioteca, DE multiclase y módulos de coexpresión opcionales.\"\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"model '{model}' no soportado.\")\n",
    "\n",
    "    # ====== OPCIONALES (no aplican a 'mirna_nb' ni 'friedman1' salvo donde tenga sentido) ======\n",
    "    redundant_map = list(info.get(\"redundant_map\", []))\n",
    "    repeated_from = []\n",
    "\n",
    "    if model not in (\"madelon_like\", \"friedman1\", \"mirna_nb\"):\n",
    "        support_idx = np.flatnonzero(info[\"support\"])\n",
    "        if n_redundant > 0:\n",
    "            X, red_map_extra = _add_redundant_features(X, support_idx, n_redundant, noise_std=0.1, rng=rng)\n",
    "            redundant_map.extend(red_map_extra)\n",
    "            supp = info[\"support_with_redundant\"]\n",
    "            if supp.shape[0] < X.shape[1]:\n",
    "                supp = np.r_[supp, np.zeros(X.shape[1] - supp.shape[0], dtype=bool)]\n",
    "            for j,_ in red_map_extra:\n",
    "                supp[j] = True\n",
    "            info[\"support_with_redundant\"] = supp\n",
    "\n",
    "    if model not in (\"mirna_nb\",):\n",
    "        if n_repeated > 0:\n",
    "            X, rep = _add_repeated_features(X, n_repeated, rng)\n",
    "            repeated_from.extend(rep)\n",
    "\n",
    "        categorical_idx: List[int] = []\n",
    "        if n_categorical > 0:\n",
    "            X, categorical_idx = _add_categorical_noise(X, n_categorical, n_categories, rng)\n",
    "        else:\n",
    "            categorical_idx = info.get(\"categorical_idx\", [])\n",
    "    else:\n",
    "        categorical_idx = info.get(\"categorical_idx\", [])\n",
    "\n",
    "    if X.shape[1] < n_features and model not in (\"mirna_nb\",):\n",
    "        X = np.column_stack([X, rng.standard_normal((n_samples, n_features - X.shape[1]))])\n",
    "\n",
    "    # Actualizamos info y salidas\n",
    "    p_final = X.shape[1]\n",
    "    for key in (\"support\", \"support_with_redundant\"):\n",
    "        mask = info.get(key, None)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        if mask.shape[0] < p_final:\n",
    "            pad = np.zeros(p_final - mask.shape[0], dtype=bool)\n",
    "            mask = np.r_[mask, pad]\n",
    "        else:\n",
    "            mask = mask[:p_final]\n",
    "        info[key] = mask\n",
    "\n",
    "    info[\"redundant_map\"] = redundant_map\n",
    "    info[\"repeated_from\"] = repeated_from\n",
    "    info[\"categorical_idx\"] = categorical_idx\n",
    "    info[\"n_features_final\"] = p_final\n",
    "\n",
    "    # Tipo de retorno (mantener conteos como int si es miRNA)\n",
    "    if info.get(\"is_counts\", False):\n",
    "        return X.astype(int), y, info\n",
    "    else:\n",
    "        return X.astype(float), y, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y, info = make_benchmark_dataset(\n",
    "    n_samples=900,\n",
    "    n_features=200,\n",
    "    n_informative=10,\n",
    "    n_classes=4, \n",
    "    class_probs=[0.1, 0.2, 0.3, 0.4],\n",
    "    task=\"classification\",                # o \"regression\"\n",
    "    model=\"sparse_logistic\",              # 'sparse_logistic', 'sparse_linear', 'friedman1', 'xor', 'parity', 'madelon_like'\n",
    "    corr=\"toeplitz\",                      # 'independent', 'toeplitz', 'block'\n",
    "    rho=0.5,                              # intensidad de correlación\n",
    "    n_blocks=5,                           # si corr='block'\n",
    "    snr=5.0,                              # para regresión\n",
    "    effect_size=1.0,\n",
    "    heteroscedastic=0.0,                  # >0 para ruido heteroscedástico en regresión\n",
    "    class_balance=0.5,                    # prevalencia deseada en clasificación\n",
    "    n_redundant=5,                        # combinaciones lineales extra de las informativas\n",
    "    n_repeated=2,                         # duplicados exactos de columnas\n",
    "    n_categorical=3, n_categories=4,      # variables categóricas de ruido (0..K-1)\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4b4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X, y, info = make_benchmark_dataset(\n",
    "    n_samples=2000, n_features=200,  # 5 base + 15 redundantes + 480 ruido (aprox.)\n",
    "    task=\"classification\", model=\"madelon_like\",\n",
    "    random_state=3,\n",
    "    n_classes=3, \n",
    "    class_probs=[1/3, 1/3, 1/3]\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "df.to_csv('madelon_like_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "687e6af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'madelon_like',\n",
       " 'task': 'classification',\n",
       " 'support': array([ True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False]),\n",
       " 'support_with_redundant': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False]),\n",
       " 'coef': None,\n",
       " 'coef_matrix': None,\n",
       " 'snr': None,\n",
       " 'noise_std': 1.0,\n",
       " 'corr': 'independent',\n",
       " 'rho': 0.0,\n",
       " 'n_blocks': None,\n",
       " 'redundant_map': [(5, [0, 1, 2, 3, 4]),\n",
       "  (6, [0, 1, 2, 3, 4]),\n",
       "  (7, [0, 1, 2, 3, 4]),\n",
       "  (8, [0, 1, 2, 3, 4]),\n",
       "  (9, [0, 1, 2, 3, 4]),\n",
       "  (10, [0, 1, 2, 3, 4]),\n",
       "  (11, [0, 1, 2, 3, 4]),\n",
       "  (12, [0, 1, 2, 3, 4]),\n",
       "  (13, [0, 1, 2, 3, 4]),\n",
       "  (14, [0, 1, 2, 3, 4]),\n",
       "  (15, [0, 1, 2, 3, 4]),\n",
       "  (16, [0, 1, 2, 3, 4]),\n",
       "  (17, [0, 1, 2, 3, 4]),\n",
       "  (18, [0, 1, 2, 3, 4]),\n",
       "  (19, [0, 1, 2, 3, 4])],\n",
       " 'n_classes': 3,\n",
       " 'class_probs': array([0.33333333, 0.33333333, 0.33333333]),\n",
       " 'notes': 'Clústeres en hipercubo 5D; 5 informativas + 15 combinaciones lineales + ruido, con 3 clases.',\n",
       " 'repeated_from': [],\n",
       " 'categorical_idx': [],\n",
       " 'n_features_final': 200}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4730e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc, yc, info_c = make_benchmark_dataset(\n",
    "    n_samples=600, n_features=300, n_informative=20,\n",
    "    model=\"mirna_nb\",\n",
    "    n_classes=3, \n",
    "    class_probs=[1/3, 1/3, 1/3],\n",
    "    # Intensidades y biblioteca\n",
    "    mirna_mean_log_expr=1.5, mirna_sd_log_expr=1.0,\n",
    "    mirna_library_log_mean=11.0, mirna_library_log_sd=0.5,\n",
    "    # Dispersión y efecto diferencial\n",
    "    mirna_dispersion_scale=1.0,\n",
    "    effect_size=0.8,   # ~desv. típica de log-FC por clase en genes informativos\n",
    "    # Coexpresión opcional\n",
    "    n_modules=5, module_sd=0.3,\n",
    "    # Dropout (normalmente 0 en miRNA; >0 para imitar scRNA)\n",
    "    dropout_rate=0.0,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "df_c = pd.DataFrame(Xc)\n",
    "df_c['target'] = yc\n",
    "df_c.to_csv('mirna_nb_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ceeed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  10,  36,  38,  50,  57,  58,  71, 115, 141, 172, 188, 215,\n",
       "       236, 237, 249, 273, 279, 280, 288])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_positions = np.where(info_c[\"support\"])[0]\n",
    "true_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0af50d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  10,  36,  38,  50,  57,  58,  71, 115, 141, 172, 188, 215,\n",
       "       236, 237, 249, 273, 279, 280, 288])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_positions = np.where(info_c[\"support_with_redundant\"])[0]\n",
    "true_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9a68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformals3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
